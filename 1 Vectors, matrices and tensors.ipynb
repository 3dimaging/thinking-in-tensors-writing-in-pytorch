{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thinking in tensors, writing in PyTorch\n",
    "\n",
    "A hands-on course by [Piotr Migdał](https://p.migdal.pl) (2019).\n",
    "\n",
    "\n",
    "## Notebook 1: Tensors\n",
    "\n",
    "**WORK IN PROGRESS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Linear algebra is the language of deep learning... and quantum mechanics.\n",
    "\n",
    "Note: in physics and engineering, tensor is not any array. There is a one-two-many rule: \n",
    "\n",
    "* 0: scalar\n",
    "* 1: vector\n",
    "* 2: matrix\n",
    "* 3 and above: n-dimensional tensor\n",
    "\n",
    "In theory, tensors can be of an arbitrarily high dimension. In deep learning, they rare exceed 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalar\n",
    "\n",
    "Scalar is \"just a number\". Real-world examples of a scalar are: temperature, pressure, price of an apple in a given shop, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tensor(42.)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Food for thought\n",
    "\n",
    "> The scalar fallacy is the false but pervasive assumption that real-world things (hotels, sandwiches, people, mutual funds, chemo drugs, whatever) have some single-dimension ordering of \"goodness\".\n",
    "\n",
    "> When you project a multi-dimensional space down to one dimension, you are involving a lot of context and preferences in the act of projecting. - [rlucas on HN](https://news.ycombinator.com/item?id=8132525)\n",
    "\n",
    "See also: [Scalar fallacy](http://observationalepidemiology.blogspot.com/2011/01/scalar-fallacy.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector\n",
    "\n",
    "Vector is an ordered list of numbers, such as `[-5., 2., 0.]`.\n",
    "\n",
    "In physics and mechanical engineering, not everything is a vector:\n",
    "\n",
    "> it is not generally true that any three numbers form a vector. It is true only if, when we rotate the coordinate system, the components of the vector transform among themselves in the correct way. - [II 02: Differential Calculus of Vector Fields](http://www.feynmanlectures.caltech.edu/II_02.html) from [The Feynman Lectures on Physics](http://www.feynmanlectures.caltech.edu/)\n",
    "\n",
    "* position\n",
    "* velocity\n",
    "* electric field\n",
    "* spatial gradient of a scalar field ($\\nabla T$)\n",
    "\n",
    "\n",
    "In deep learning we are more... relaxed. Usually vectors are abstract, \n",
    "\n",
    "\n",
    "* feature vector after a ImageNet-trained vector\n",
    "* a word representation in (see: [king - man + woman is queen; but why?](https://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html))\n",
    "* user and product vectors in [Factorization Machines](https://www.reddit.com/r/MachineLearning/comments/65d3lt/r_factorization_machines_2010_a_classic_paper_in/) and related recommendation systems\n",
    "\n",
    "\n",
    "$$\\vec{v} = \\left[ v_1, v_2, \\ldots, v_n \\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = tensor([1.5, -0.5, 3.0])\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v.dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector arithmetics\n",
    "\n",
    "We can multiply vectors by a scalar: \n",
    "\n",
    "$$a \\vec{v} = \\left[a v_1, a v_2, \\ldots, a v_n \\right]$$\n",
    "\n",
    "Or, provided that two vectors have the same lenth, add and substract vectors to each other:\n",
    "\n",
    "$$\\vec{v} + \\vec{u}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2 * v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = tensor([1., 0., 1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v + u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector length\n",
    "\n",
    "\n",
    "$$|\\vec{v}| = \\sqrt{v_1^2 + v_2^2 + \\ldots + v_n^2} = \\sqrt{\\sum_{i=1}^n v_i^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v.pow(2).sum().sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.pow(v, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v.pow(2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v / v.norm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix\n",
    "\n",
    "Typical operations:\n",
    "\n",
    "* rotation\n",
    "* next step in a stochastic process\n",
    "* scalar products\n",
    "\n",
    "\n",
    "Give example with colors to $RGB$ to $black/R-G$\n",
    "\n",
    "https://xkcd.com/184/\n",
    "\n",
    "* [Hessian matrix](https://en.wikipedia.org/wiki/Hessian_matrix) of a scalar\n",
    "\n",
    "Add:\n",
    "\n",
    "* matrix as in Quantum Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = tensor([[1., 2.], [3., 4.]])\n",
    "M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M.matmul(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor([1., 0.]).matmul(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Python 3.5+\n",
    "M @ M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M * M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor([1., 2.]).matmul(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M.svd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M.det()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor\n",
    "\n",
    "\n",
    "Tensor is a generalization of vectors and matrices for more dimensions.\n",
    "\n",
    "In physics and engineering they have more properties, as in:\n",
    "\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/StressEnergyTensor_contravariant.svg/250px-StressEnergyTensor_contravariant.svg.png)\n",
    "\n",
    "[Electromagnetic tensor](https://en.wikipedia.org/wiki/Electromagnetic_tensor) from [Introduction to the mathematics of general relativity - Wikipedia](https://en.wikipedia.org/wiki/Introduction_to_the_mathematics_of_general_relativity), see also: [Tensor](https://en.wikipedia.org/wiki/Tensor).\n",
    "\n",
    "In deep learning, there are any arrays.\n",
    "You can look at See also:\n",
    "\n",
    "* [Einsum is All you Need - Einstein Summation in Deep Learning - Tim Rocktäschel](https://rockt.github.io/2018/04/30/einsum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To dos\n",
    "\n",
    "(Internal notes...)\n",
    "\n",
    "Technicalities:\n",
    "\n",
    "* avoid Python loops \n",
    "* data type\n",
    "* (un)squeeze\n",
    "* stride\n",
    "* from/to GPU\n",
    "\n",
    "Advanced:\n",
    "\n",
    "* Einstein summation\n",
    "* Tensor diagrams\n",
    "\n",
    "To do:\n",
    "\n",
    "* links\n",
    "* LaTeX formulas to compare \n",
    "* More practical examples\n",
    "\n",
    "Extras:\n",
    "\n",
    "* SVG diagrams for tensors (as in: https://jsfiddle.net/stared/8huz5gy7/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
