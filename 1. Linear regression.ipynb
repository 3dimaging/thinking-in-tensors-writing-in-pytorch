{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<em>Prepared by [Weronika Ormaniec](https://github.com/werkaaa)</em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple [linear regression](https://en.wikipedia.org/wiki/Linear_regression)\n",
    " tries to model the dependence between explanatory variable and dependent variable by matching a proper line into the analysed data. Before attepmting to use linear regression one should determine if the variables depend on each other and if their relationship is linear (or may be scaled into a linear one).\n",
    "![https://upload.wikimedia.org/wikipedia/commons/3/34/Correlation_coefficient.png](https://upload.wikimedia.org/wikipedia/commons/3/34/Correlation_coefficient.png)\n",
    "![https://imgs.xkcd.com/comics/linear_regression.png](https://imgs.xkcd.com/comics/linear_regression.png)\n",
    "\n",
    "Sometimes, linear regression is not a propper approach to the analysed problem.\n",
    "![https://imgs.xkcd.com/comics/extrapolating.png](https://imgs.xkcd.com/comics/extrapolating.png)\n",
    "\n",
    "However, it occurs that lots of dependencies in the actual world can be described just by fitting a linear equation to the observed data. That's what we are going to do now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from livelossplot import PlotLosses\n",
    "from ipywidgets import interact, fixed\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/Animals.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Have you ever wondered what is the relation between brain and body weights among various animal species?\n",
    "We will try to find a solution to this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot.scatter(x=\"BodyWeight(kg)\", y=\"BrainWeight(kg)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance it does not resemble any particular dependance. However, if we change the scale something interesting can be spotted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot.scatter(x=\"BodyWeight(kg)\", y=\"BrainWeight(kg)\", logx=True, logy=True)\n",
    "#plt.scatter(np.log(data['BodyWeight(kg)']), np.log(data['BrainWeight(kg)']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can say that the data points form a line. Then, we can try to find the equation of that line.\n",
    "\n",
    "That is why we need linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 Linear equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider two sets of numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1, 2, 3, 4]\n",
    "y = [3.2, 5.1, 6.9, 9.3]\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the scatterplot it can be easly seen that the relationship between presented data is almost linear. We will try to apply the equation:\n",
    "\n",
    "$$ y = w_0 + w_1x $$\n",
    "\n",
    "to the analysed dataset. The only problem is how to find $w_0$ and $w_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Manual approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to find a proper line manually!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model(b, a, x, y):\n",
    "    y_pred = b + a * np.array(x) \n",
    "    plt.scatter(x, y)\n",
    "    plt.plot(x, y_pred, 'g')\n",
    "    plt.show()\n",
    "    print('a =', a)\n",
    "    print('b =', b)\n",
    "    \n",
    "interact(plot_model, \n",
    "         a=(-2.5, 2.5), \n",
    "         b=(-2.5, 2.5),\n",
    "         x=fixed(x),\n",
    "         y=fixed(y)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to somehow measure if the coefficients in the equation are good enough to describe our problem. In order to do it we will define a loss function - an equation that will tell us how much our approximation differs from the expected output. \n",
    "\n",
    "The loss function should:\n",
    "* depend only on the coefficients of the model, expected output and our approximation,\n",
    "* shrink if our approximation is becomming better and grow if it gets worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes to linear regression the most common approach is the least-squares loss function. We will calculate the average square of the vertical deviations from each data point to the line. Since we first square the dviations, it does not matter if the data point is above or below the line. \n",
    "\n",
    "\n",
    "$$y^{ pred}_{i} = ax_{i}+b $$\n",
    "$$L=\\frac{1}{N}\\sum_{i=0}^N( y^{pred}_{i} - y_{i})^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will get random $a$ and $b$ and apply our loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.randn(1,2)\n",
    "X = torch.tensor([[1.0,1.0,1.0,1.0], x])\n",
    "Y = torch.tensor(y)\n",
    "print(W)\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Y_pred(W, X):\n",
    "     return W.matmul(X) #matrix multiplication\n",
    "Y_pred(W, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Loss(W, X, Y):\n",
    "    y_pred = Y_pred(W, X)\n",
    "    return ((y_pred-Y)*(y_pred-Y)).mean()\n",
    "Loss(W, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Minimazing loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will find more detailed explanation of what will happen here in the chapter \"Gradient descent\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have defined the loss function, we should minimize it. By doing so we will step by step rotate and move the line, so it will reflect the actual location of data points. In order to do it we need to repeatedly shift the weights till we find a minimum of the loss function. What we need is a mathematical operation that will tell us how the loss function will change, if we increase or decreas $w_0$ and $w_1$. The operation we are looking for is partial derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\dfrac{\\partial L}{\\partial b} = \\frac{2}{N}\\sum_{i=0}^N (y^{pred}_{i} -y_{i})$$\n",
    "\n",
    "\n",
    "$$\\dfrac{\\partial L}{\\partial a}  = \\frac{2}{N}\\sum_{i=0}^N (y^{pred}_{i} -y_{i}) \\cdot x_{i}$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dL_dw0 (W, X, Y):\n",
    "    y_pred = Y_pred(W, X)\n",
    "    return 2*(y_pred - Y).mean()\n",
    "dL_dw0(W, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dL_dw1 (W, X, Y):\n",
    "    y_pred = Y_pred(W, X)\n",
    "    return 2*((y_pred - Y)*X[1]).mean()\n",
    "dL_dw1(W, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two more thing we have to specify is **learning\\_rate** - hyperparamiter that will define how much the value of the derivative will influance the change of $w_0$ and $w_1$ and **num\\_epochs** - hyperparameter defining how many iterations it will take to sufficiently minimiaze the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_step(W, X, Y, learnig_rate):\n",
    "    W[0][0]-=learnig_rate*dL_dw0(W, X, Y)\n",
    "    W[0][1]-=learnig_rate*dL_dw1(W, X, Y)\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimise_loss_function(W, X, Y, learning_rate, num_epochs):\n",
    "    loss_history = []\n",
    "    for i in range(num_epochs):\n",
    "        W = gradient_step(W, X, Y, learning_rate)\n",
    "        loss_history.append(Loss(W, X, Y))\n",
    "    return W, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(W)\n",
    "\n",
    "learning_rate = 0.1\n",
    "num_epochs = 30\n",
    "W_trained, loss_history = minimise_loss_function(W, X, Y, learning_rate, num_epochs)\n",
    "\n",
    "print(W_trained, loss_history)\n",
    "plt.plot(list(range(num_epochs)), loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = W.numpy()[0][0]\n",
    "a = W.numpy()[0][1]\n",
    "plot_model(b, a, np.array(x), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that is how we find the propper line!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Linear regression using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing how linear regression works, let's come back to the relation between body and brain weights. This time we will use built-in PyTorch functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we need to prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(np.log(data['BodyWeight(kg)']))\n",
    "Y = torch.tensor(np.log(data['BrainWeight(kg)']))\n",
    "X = X.view(1, 27, 1)\n",
    "Y = Y.view(1, 27, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of initializing the coefficients manually, we can define the model using a built in class. Since both input and output in the analysed problem have only one dimension we set **(1,1)** as arguments of **nn.Linear**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(1, 1)\n",
    "print(model.weight)\n",
    "print(model.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insted of **gradient\\_step** function, we will define an **optimizer** with learning rate and built-int **loss function**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = F.mse_loss\n",
    "loss = loss_function(model(X), Y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training the model, let's see what does the line with random cefficients look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_viewed = X.view(27)\n",
    "plot_model(model.bias.item(), model.weight.item(), X_viewed.numpy(), Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the model - minimise the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liveloss = PlotLosses()\n",
    "\n",
    "def train(num_epochs, X, Y, model, loss_function, optim):\n",
    "    loss_history = []\n",
    "    preds = torch.tensor([])\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        Y_pred = model(X)\n",
    "        loss = loss_function(Y_pred, Y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        preds = torch.cat([preds, Y_pred], 0)\n",
    "        \n",
    "        epoch_loss = loss.data.item()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(X)\n",
    "\n",
    "        liveloss.update({\n",
    "            'loss': avg_loss\n",
    "        })\n",
    "        liveloss.draw()\n",
    "        print(\"b =\", model.bias[0].item())\n",
    "        print(\"a =\", model.weight[0][0].item())\n",
    "        \n",
    "    return preds\n",
    "\n",
    "predictions = train(201, X, Y, model, loss_function, optim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see, how the line was changing during learning process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_viewed = X.view(27)\n",
    "predictions = predictions.view([-1, 27])\n",
    "plt.scatter(X, Y)\n",
    "for i in range(predictions.size()[0]):\n",
    "    if i%10 == 0:\n",
    "        plt.plot(X_viewed.numpy(),predictions[i].detach().numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we fitted the final line properly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_viewed = X.view(27)\n",
    "Y_viewed = Y.view(27)\n",
    "plot_model(model.bias.item(), model.weight.item(), X_viewed.numpy(), Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It fits the data much better than at the begining. We have found the relation between brain and body weights among various animal species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_n = predictions[-1].exp().detach().numpy()\n",
    "df = pd.DataFrame({'Species'  : data.index, 'PredictedBrainWeight(kg)': predictions_n})\n",
    "df.index = df[\"Species\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_p = pd.concat([data, df['PredictedBrainWeight(kg)']], axis=1)\n",
    "data_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compare predicted brain weights with actual data. What does it mean that the actual brain weight is bigger than predicted one? Is an animal more clever in that case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to practice linear regression, here is another dataset. It describes the relation between weight and average heart rate of various animals. \n",
    "\n",
    "(Tip: try to scale the data, by taking logarithm of both values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv(\"data/Heart_rate_and_weight.csv\", index_col=0)\n",
    "data2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here are some interesting websites on the subject of linear regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* [Linear regression](http://www.stat.yale.edu/Courses/1997-98/101/linreg.htm)\n",
    "* [Ordinary Least Squares Regression-Explained Visually](http://setosa.io/ev/ordinary-least-squares-regression/)\n",
    "* [Pearson correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
