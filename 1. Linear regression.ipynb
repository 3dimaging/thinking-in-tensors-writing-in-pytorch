{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Animals.csv\", )\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Have you ever wondered what is the relation between brain and body weights among various animal species?\n",
    "We will try to find a solution to this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot.scatter(x=\"BodyWeight(kg)\", y=\"BrainWeight(kg)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance it does not resemble any particular dependance. However, if we change the scale smoatehing interesting can be spotted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot.scatter(x=\"BodyWeight(kg)\", y=\"BrainWeight(kg)\", logx=True, logy=True)\n",
    "#plt.scatter(np.log(data['BodyWeight(kg)']), np.log(data['BrainWeight(kg)']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can say that the data points form a line. Then, we can try to find the equation of that line.\n",
    "\n",
    "That is why we need linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 Linear equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider two sets of numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1, 2, 3, 4]\n",
    "y = [3.2, 5.1, 6.9, 9.3]\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the scatterplot it can be easly seen that the relationship between presented data is almost linear. It occurs that lots of dependencies in the actual world can be described just by fitting a linear equation to the observed data. We will try to apply the equation:\n",
    "\n",
    "$$ y = w_0 + w_1x $$\n",
    "\n",
    "to the analysed dataset. The only problem is how to find $w_0$ and $w_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to somehow measure if the coefficients in the equation are good enough to describe our problem. In order to do it we will define a loss function - an equation that will tell us how much our approximation differs from the expected output. \n",
    "\n",
    "The loss function should:\n",
    "* depend only on the coefficients of the model, expected output and our approximation,\n",
    "* shrink if our approximation is becomming better and grow if it gets worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes to linear regression the most common approach is the least-squares loss function. We will calculate the average square of the vertical deviations from each data point to the line. Since we first square the dviations, it does not matter if the data point is above or below the line. \n",
    "\n",
    "\n",
    "$$y^{ pred}_{i} = w_0 + w_1x_{i} $$\n",
    "$$L=\\frac{1}{N}\\sum_{i=0}^N( y^{pred}_{i} - y_{i})^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will get random $w_0$ and $w_1$ and apply our loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.randn(1,2)\n",
    "X = torch.tensor([[1.0,1.0,1.0,1.0], x])\n",
    "Y = torch.tensor(y)\n",
    "print(W)\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Y_pred(W, X):\n",
    "     return torch.mm(W,X) #matrix multiplication\n",
    "Y_pred(W, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Loss(W, X, Y):\n",
    "    y_pred = Y_pred(W, X)\n",
    "    return torch.mean((y_pred-Y)*(y_pred-Y))\n",
    "Loss(W, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Minimazing loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will find more detailed explanation of what will happen here in the chapter \"Gradient descent\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have defined the loss function, we should minimize it. By doing so we will step by step rotate and move the line, so it will reflect the actual location of data points. In order to do it we need to repeatedly shift the weights till we find a minimum of the loss function. What we need is a mathematical operation that will tell us how the loss function will change, if we increase or decreas $w_0$ and $w_1$. The operation we are looking for is partial derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\dfrac{\\partial L}{\\partial w_0} = \\frac{2}{N}\\sum_{i=0}^N (y^{pred}_{i} -y_{i})$$\n",
    "\n",
    "\n",
    "$$\\dfrac{\\partial L}{\\partial w_1}  = \\frac{2}{N}\\sum_{i=0}^N (y^{pred}_{i} -y_{i}) \\cdot x_{i}$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dL_dw0 (W, X, Y):\n",
    "    y_pred = Y_pred(W, X)\n",
    "    return 2*torch.mean(y_pred - Y)\n",
    "dL_dw0(W, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dL_dw1 (W, X, Y):\n",
    "    y_pred = Y_pred(W, X)\n",
    "    return 2*torch.mean((y_pred - Y)*X[1])\n",
    "dL_dw1(W, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two more thing we have to specify is **learning\\_rate** - hyperparamiter that will define how much the value of the derivative will influance the change of $w_0$ and $w_1$ and **num\\_epochs** - hyperparameter defining how many iterations it will take to sufficiently minimiaze the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_step(W, X, Y, learnig_rate):\n",
    "    W[0][0]-=learnig_rate*dL_dw0(W, X, Y)\n",
    "    W[0][1]-=learnig_rate*dL_dw1(W, X, Y)\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimise_loss_function(W, X, Y, learning_rate, num_epochs):\n",
    "    loss_history = []\n",
    "    for i in range(num_epochs):\n",
    "        W = gradient_step(W, X, Y, learning_rate)\n",
    "        loss_history.append(Loss(W, X, Y))\n",
    "    return W, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(W)\n",
    "\n",
    "learning_rate = 0.1\n",
    "num_epochs = 30\n",
    "W_trained, loss_history = minimise_loss_function(W, X, Y, learning_rate, num_epochs)\n",
    "\n",
    "print(W_trained, loss_history)\n",
    "plt.plot(list(range(epoch)), loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_0 = W.numpy()[0][0]\n",
    "x_a = np.array(x)\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x_a, w_1*x_a+w_0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that is how we find the propper line!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Linear regression using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing how linear regression works, let's come back to the relation between body and brain weights. This time we will use built-in PyTorch functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we need to prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(np.log(data['BodyWeight(kg)']))\n",
    "Y = torch.tensor(np.log(data['BrainWeight(kg)']))\n",
    "X = X.view(1, 27, 1)\n",
    "Y = Y.view(1, 27, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of initializing the coefficients manually, we can define the model using a built in class. Since both input and output in the analysed problem have only one dimension we set **(1,1)** as arguments of **nn.Linear**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(1, 1)\n",
    "print(model.weight)\n",
    "print(model.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insted of **gradient\\_step** function, we will define an **optimizer** with learning rate and built-int **loss function**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = F.mse_loss\n",
    "loss = loss_function(model(X), Y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the model - minimise the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs, X, Y, model, loss_function, optim):\n",
    "    for epoch in range(num_epochs):\n",
    "        Y_pred = model(X)\n",
    "        loss = loss_function(Y_pred, Y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "        \n",
    "    print('Training loss: ', loss_function(model(X), Y))\n",
    "\n",
    "train(5000, X, Y, model, loss_function, optim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we fitted the line properly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.view(27)\n",
    "Y = Y.view(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, Y)\n",
    "plt.plot(X.numpy(),model.weight.item()*X.numpy()+model.bias.item())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have found the relation between brain and body weights among various animal species. We also discussed how linear regression works, applied it step by step and using PyTorch built-ins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here are some interesting websites on the subject of linear regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* [Linear regression](http://www.stat.yale.edu/Courses/1997-98/101/linreg.htm)\n",
    "* [Ordinary Least Squares Regression-Explained Visually](http://setosa.io/ev/ordinary-least-squares-regression/)\n",
    "* [Pearson correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
