{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thinking in tensors, writing in PyTorch\n",
    "\n",
    "A hands-on course by [Piotr MigdaÅ‚](https://p.migdal.pl) (2019).\n",
    "\n",
    "## Notebook 3: Gradient descent\n",
    "\n",
    "\n",
    "> X: I want to learn Deep Learning.  \n",
    "> Me: Do you know what is gradient?  \n",
    "> X: Yes  \n",
    "> Me: Then, it an easy way downhill!\n",
    "\n",
    "Memic content:\n",
    "\n",
    "* https://twitter.com/jebbery/status/995491957559439360\n",
    "* https://twitter.com/smaine/status/994723834434502658\n",
    "\n",
    "\n",
    "**CONTENT MORE OR LESS THERE, NEEDS DESCRIPTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$y = x^2$$\n",
    "\n",
    "$$ \\frac{\\partial y}{\\partial x} = 2 x$$\n",
    "\n",
    "For $y^2$ we can calculate it:\n",
    "\n",
    "$$\\lim_{x \\to 0} \\frac{y(x + h) - y(x)} {h}$$\n",
    "\n",
    "Limit is a mathematical tool for \n",
    "\n",
    "$$\\frac{x^2 + 2 x h + h^2 - x^2}{h} = 2 x + h $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace(-4, 4, num=100)\n",
    "Y = X**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax0, ax1) = plt.subplots(nrows=2, ncols=1,\n",
    "                               sharex=True, figsize=(7, 4))\n",
    "\n",
    "ax0.plot(X, Y)\n",
    "ax0.set(title='', xlabel='', ylabel='y')\n",
    "\n",
    "ax1.plot(X, 2 * X)\n",
    "ax1.set(title='', xlabel='x', ylabel='dy/dx')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical derivative in NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can go it automatically\n",
    "plt.plot((X[1:] + X[:-1]) / 2, np.diff(Y) / np.diff(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symbolic derivative in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(10., requires_grad=True)\n",
    "y = x.pow(2)\n",
    "y.backward()\n",
    "\n",
    "# y\n",
    "y\n",
    "\n",
    "# dy / dx\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.2\n",
    "x0 = 4.\n",
    "\n",
    "xs = [x0]\n",
    "x = torch.tensor(x0, requires_grad=True)\n",
    "\n",
    "for i in range(10):\n",
    "    y = x.pow(2)\n",
    "    y.backward()\n",
    "    x.data.add_(- lr * x.grad.data)\n",
    "    x.grad.data.zero_()\n",
    "    xs.append(x.item())\n",
    "\n",
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_X = np.array(xs)\n",
    "points_Y = points_X**2\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(7, 4))\n",
    "ax.plot(X, Y)\n",
    "ax.plot(points_X, points_Y, '-')\n",
    "ax.plot(points_X, points_Y, 'r.')\n",
    "ax.set(title='Gradient descent', xlabel='x', ylabel='y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Try other learning rates, e.g.:\n",
    "\n",
    "* 0.1\n",
    "* 0.5\n",
    "* 0.75\n",
    "* 1.\n",
    "* 1.5\n",
    "* -0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slightly more complicated functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x - 4 * x**2 + 0.25 * x**4\n",
    "\n",
    "def df(x):\n",
    "    return 1 - 8 * x + x**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace(-4, 4, num=100)\n",
    "\n",
    "fig, (ax0, ax1) = plt.subplots(nrows=2, ncols=1,\n",
    "                               sharex=True, figsize=(7, 4))\n",
    "\n",
    "ax0.plot(X, f(X))\n",
    "ax0.set(title='', xlabel='', ylabel='y')\n",
    "\n",
    "ax1.plot(X, df(X))\n",
    "ax1.hlines(y=0, xmin=-4, xmax=4, linestyles='dashed')\n",
    "ax1.set(title='', xlabel='x', ylabel='dy/dx')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "x0 = 4.\n",
    "\n",
    "xs = [x0]\n",
    "x = torch.tensor(x0, requires_grad=True)\n",
    "\n",
    "for i in range(10):\n",
    "    y = f(x)\n",
    "    y.backward()\n",
    "    x.data.add_(- lr * x.grad.data)\n",
    "    x.grad.data.zero_()\n",
    "    xs.append(x.item())\n",
    "\n",
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_X = np.array(xs)\n",
    "points_Y = f(points_X)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(7, 4))\n",
    "ax.plot(X, f(X))\n",
    "ax.plot(points_X, points_Y, '-')\n",
    "ax.plot(points_X, points_Y, 'r.')\n",
    "ax.set(title='Gradient descent', xlabel='x', ylabel='y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient in 2d\n",
    "\n",
    "Gradients make sense for more dimensions. For mountains, gradient would be a vector directed along to the biggest slope.\n",
    "\n",
    "\n",
    "\n",
    "For example, let's have a function:\n",
    "\n",
    "$$y = g(x_1, x_2) = x_1^2 + \\sin(x_2)$$\n",
    "\n",
    "In this case, gradient is a vector. To calculate gradient we use [partial derviative](https://en.wikipedia.org/wiki/Partial_derivative).\n",
    "\n",
    "$$\\nabla g = \\left( \\frac{\\partial g}{\\partial x_1}, \\frac{\\partial g}{\\partial x_2} \\right) = \\left( 2 x_1, \\cos(x_2) \\right) $$\n",
    "\n",
    "\n",
    "Gradient symbol:\n",
    "\n",
    "$$\\nabla = \\left( \\frac{\\partial }{\\partial x_1}, \\frac{\\partial }{\\partial x_2} \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X0_ = np.linspace(-4, 4, num=100)\n",
    "X1_ = np.linspace(-4, 4, num=100)\n",
    "X0, X1 = np.meshgrid(X0_, X1_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# purely technically, so that we have the same code\n",
    "# for NumPy and PyTorch\n",
    "\n",
    "def sin(x):\n",
    "    if type(x) == torch.Tensor:\n",
    "        return x.sin()\n",
    "    else:\n",
    "        return np.sin(x)\n",
    "    \n",
    "def cos(x):\n",
    "    if type(x) == torch.Tensor:\n",
    "        return x.cos()\n",
    "    else:\n",
    "        return np.cos(x)\n",
    "\n",
    "# now the functions and their gradients\n",
    "# (calculated by hand)\n",
    "    \n",
    "def g(x0, x1):\n",
    "    return 0.25 * x0**2 + sin(x1)\n",
    "\n",
    "def dg_dx0(x0, x1):\n",
    "    return 0.5 * x0\n",
    "\n",
    "def dg_dx1(x0, x1):\n",
    "    return cos(x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, let's draw a [contour plot](https://en.wikipedia.org/wiki/Contour_line), well known from topographic maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = plt.contour(X0, X1, g(X0, X1), cmap='coolwarm')\n",
    "plt.clabel(cs, inline=1, fontsize=10)\n",
    "plt.title(\"g\")\n",
    "plt.xlabel(\"x0\")\n",
    "plt.ylabel(\"x1\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = plt.contour(X0, X1, dg_dx0(X0, X1), cmap='coolwarm')\n",
    "plt.clabel(cs, inline=1, fontsize=10)\n",
    "plt.title(\"dg/dx0\")\n",
    "plt.xlabel(\"x0\")\n",
    "plt.ylabel(\"x1\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = plt.contour(X0, X1, dg_dx1(X0, X1), cmap='coolwarm')\n",
    "plt.clabel(cs, inline=1, fontsize=10)\n",
    "plt.title(\"dg/dx1\")\n",
    "plt.xlabel(\"x0\")\n",
    "plt.ylabel(\"x1\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X0_less = X0[::5, ::5]\n",
    "X1_less = X1[::5, ::5]\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.set_title(r'$\\nabla g$')\n",
    "Q = ax1.quiver(X0_less, X1_less,\n",
    "               dg_dx0(X0_less, X1_less), dg_dx1(X0_less, X1_less),\n",
    "               units='width')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Gradient descent in 2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.25\n",
    "v = [-3.5, 1.0]\n",
    "\n",
    "xs = [v[0]]\n",
    "ys = [v[1]]\n",
    "v = torch.tensor(v, requires_grad=True)\n",
    "\n",
    "for i in range(20):\n",
    "    y = g(v[0], v[1])\n",
    "    y.backward()\n",
    "    v.data.add_(- lr * v.grad.data)\n",
    "    v.grad.data.zero_()\n",
    "    \n",
    "    xs.append(v[0].item())\n",
    "    ys.append(v[1].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = plt.contour(X0, X1, g(X0, X1), cmap='coolwarm')\n",
    "plt.clabel(cs, inline=1, fontsize=10)\n",
    "plt.plot(xs, ys, '-')\n",
    "plt.plot(xs, ys, 'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
