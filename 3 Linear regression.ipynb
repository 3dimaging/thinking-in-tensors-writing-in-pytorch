{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thinking in tensors, writing in PyTorch\n",
    "\n",
    "A hands-on course by [Piotr Migdał](https://p.migdal.pl) (2019).\n",
    "This notebook prepared by [Weronika Ormaniec](https://github.com/werkaaa) and Piotr Migdał.\n",
    "\n",
    "## Notebook 3: Linear regression\n",
    "\n",
    "[Linear regression](https://en.wikipedia.org/wiki/Linear_regression) is one of the most common predictive models. In plain words, we fit a straight line that fits to the data. Mathematically speaking, we use linear combination of input variables to predict the output variable.\n",
    "\n",
    "$$y = a_1 x_1 + \\ldots a_n x_n + b$$\n",
    "\n",
    "Before moving any further, try to experience it viscerally with [Ordinary Least Squares Regression\n",
    "Explained Visually - Visually Explained](http://setosa.io/ev/ordinary-least-squares-regression/):\n",
    "\n",
    "![http://setosa.io/ev/ordinary-least-squares-regression/](imgs/linreg_setosa.png)\n",
    "\n",
    "However, it occurs that lots of dependencies in the actual world can be described just by fitting a linear equation to the observed data. That's what we are going to do now!\n",
    "\n",
    "In Python we typically use [LinearRegression from scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html). Here we use PyTorch to show everything step-by-step. Moreover, linear regression is a building block of any regression with deep learning - so it is good to understand it well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from livelossplot import PlotLosses\n",
    "from ipywidgets import interact, fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Have you ever wondered what is the relation between brain and body weights among various animal species?\n",
    "\n",
    "Let's try a [Brain to Body Weight Dataset](http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_Brain2BodyWeight)!\n",
    "\n",
    "> These data represent the relation between weights of the body and brain of various species. It may be used to discuss bivariate exploratory and quantitative data analyses in the case of allometric relationships. Brain-to-body weight ratio is assumed to be related to species intelligence. The encephalization quotient is a more complex measurement that takes into account allometric effects of widely divergent body sizes across several taxa. The brain-to-body mass ratio is a simpler measure of encephalization within species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/Animals.csv\", index_col='Species')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a scatter plot\n",
    "data.plot.scatter(x=\"BodyWeight(kg)\", y=\"BrainWeight(kg)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance it does not resemble any particular dependance. However, if we change the scale something interesting can be spotted with [logarithmic scaling](https://simple.wikipedia.org/wiki/Logarithmic_scale):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot.scatter(x=\"BodyWeight(kg)\", y=\"BrainWeight(kg)\", logx=True, logy=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a clear dependence that the bigger body weight (on average) the bigger brain weight.\n",
    "Let's investigate that!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A toy example\n",
    "\n",
    "**TODO: This one or just move with the original numbers, otherwise we lose the flow**\n",
    "\n",
    "Let's consider two sets of numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1., 2., 3., 4.])\n",
    "y = np.array([3.2, 5.1, 6.9, 9.3])\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the scatterplot it can be easly seen that the relationship between presented data is almost linear. We will try to apply the equation:\n",
    "\n",
    "$$ y = ax+b$$\n",
    "\n",
    "to the analysed dataset. The only problem is how to find $a$ and $b$.\n",
    "\n",
    "Try to find a proper line manually!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model(a, b, x, y):\n",
    "    y_pred = a * x + b\n",
    "    plt.plot(x, y_pred, 'g')\n",
    "    plt.scatter(x, y)\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.ylim([-1, 12])\n",
    "    plt.show()\n",
    "    \n",
    "interact(plot_model, \n",
    "         a=(-1, 5), \n",
    "         b=(-0.5, 4),\n",
    "         x=fixed(x),\n",
    "         y=fixed(y)\n",
    "        );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "\n",
    "We will try to somehow measure if the coefficients in the equation are good enough to describe our problem. In order to do it we will define a loss function - an equation that will tell us how much our approximation differs from the expected output. \n",
    "\n",
    "The loss function should:\n",
    "\n",
    "* depend only on the coefficients of the model, expected output and our approximation,\n",
    "* shrink if our approximation is becoming better and grow if it gets worse.\n",
    "\n",
    "When it comes to linear regression the most common approach is the least-squares loss function. We will calculate the average square of the vertical deviations from each data point to the line. Since we first square the dviations, it does not matter if the data point is above or below the line. \n",
    "\n",
    "\n",
    "$$y^{ pred}_{i} = ax_{i}+b $$\n",
    "$$L=\\frac{1}{N}\\sum_{i=1}^N( y^{pred}_{i} - y_{i})^2 $$\n",
    "\n",
    "We will get random $a$ and $b$ and apply our loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa_ = np.linspace(0, 4, num=100)\n",
    "bb_ = np.linspace(-4, 8, num=100)\n",
    "aa, bb = np.meshgrid(aa_, bb_)\n",
    "\n",
    "def loss_numpy(aa, bb, x, y):\n",
    "    loss = np.zeros_like(aa)\n",
    "    for i in range(len(loss)):\n",
    "        for j in range(len(loss[0])):\n",
    "            loss[i][j] = ((aa[i,j] * x + bb[i,j] - y)**2).sum()\n",
    "    return loss\n",
    "\n",
    "cs = plt.contour(aa, bb, np.sqrt(loss_numpy(aa, bb, x, y)), cmap='coolwarm')\n",
    "plt.clabel(cs, inline=1, fontsize=10)\n",
    "plt.title(\"Loss\")\n",
    "plt.xlabel('a')\n",
    "plt.ylabel('b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.randn(1,2)\n",
    "X = torch.tensor([[1.0,1.0,1.0,1.0], x]).float()  # NO, we want a and b separatly, without such tricks!\n",
    "Y = torch.tensor(y).float()\n",
    "\n",
    "# TO FIX: No consequti\n",
    "print(W)\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Y_pred(W, X):\n",
    "     return W.matmul(X) # matrix multiplication\n",
    "\n",
    "Y_pred(W, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: only loss, without prediction \n",
    "# TODO: loss LOWERCASE\n",
    "def Loss(W, X, Y):\n",
    "    y_pred = Y_pred(W, X)\n",
    "    return (y_pred - Y).pow(2).mean()\n",
    "Loss(W, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimizing loss function\n",
    "\n",
    "**TODO: Write that for lin reg there is an explicit formula, and write so.**\n",
    "\n",
    "You will find more detailed explanation of what will happen here in the chapter \"Gradient descent\".\n",
    "\n",
    "Since we have defined the loss function, we should minimize it. By doing so we will step by step rotate and move the line, so it will reflect the actual location of data points. In order to do it we need to repeatedly shift the weights till we find a minimum of the loss function. What we need is a mathematical operation that will tell us how the loss function will change, if we increase or decrease $a$ and $b$. The operation we are looking for is partial derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\dfrac{\\partial L}{\\partial a}  = \\frac{2}{N}\\sum_{i=0}^N (y^{pred}_{i} -y_{i}) \\cdot x_{i}$$ \n",
    "\n",
    "$$\\dfrac{\\partial L}{\\partial b} = \\frac{2}{N}\\sum_{i=0}^N (y^{pred}_{i} -y_{i})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dL_da(W, X, Y):\n",
    "    y_pred = Y_pred(W, X)\n",
    "    return 2*((y_pred - Y)*X[1]).mean()\n",
    "\n",
    "dL_da(W, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dL_db(W, X, Y):\n",
    "    y_pred = Y_pred(W, X)\n",
    "    return 2*(y_pred - Y).mean()\n",
    "\n",
    "dL_db(W, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two more things we have to specify is **learning\\_rate** - hyperparamiter that will define how much the value of the derivative will influance the change of $a$ and $b$ and **num\\_epochs** - hyperparameter defining how many iterations it will take to sufficiently minimiaze the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: slightly too much abstraction nonsese\n",
    "# it IMHO it shoudn't be a separe function\n",
    "def gradient_step(W, X, Y, learnig_rate):\n",
    "    W[0][0] -= learnig_rate * dL_db(W, X, Y)\n",
    "    W[0][1] -= learnig_rate * dL_da(W, X, Y)\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimise_loss_function(W, X, Y, learning_rate, num_epochs):\n",
    "    loss_history = []\n",
    "    for i in range(num_epochs):\n",
    "        W = gradient_step(W, X, Y, learning_rate)\n",
    "        loss_history.append(Loss(W, X, Y))\n",
    "    return W, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(W)  # TO FIX: \n",
    "\n",
    "learning_rate = 0.1\n",
    "num_epochs = 30\n",
    "W_trained, loss_history = minimise_loss_function(W, X, Y, learning_rate, num_epochs)\n",
    "\n",
    "print(W_trained, loss_history)  # TO FIX: Please. no.\n",
    "plt.plot(list(range(num_epochs)), loss_history)  # use livelossplot instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = W.numpy()[0][0]\n",
    "a = W.numpy()[0][1]\n",
    "plot_model(b, a, np.array(x), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that is how we find the propper line!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression using PyTorch\n",
    "\n",
    "Knowing how linear regression works, let's come back to the relation between body and brain weights. This time we will use built-in PyTorch functions.\n",
    "\n",
    "Firstly, we need to prepare the data. \n",
    "\n",
    "**TO DO: How? Explain steps in plain English.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(np.log(data['BodyWeight(kg)']))\n",
    "Y = torch.tensor(np.log(data['BrainWeight(kg)']))\n",
    "X = X.view(1, 27, 1)  # TOFIX: never use hardcoded sample size\n",
    "Y = Y.view(1, 27, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of initializing the coefficients manually, we can define the model using a built in class. Since both input and output in the analysed problem have only one dimension we set **(1,1)** as arguments of **nn.Linear**.\n",
    "\n",
    "**TODO: Nope. It's an abstraction rocket jump. Plus: changing a dataset AND method in no good.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(1, 1)\n",
    "print(model.weight)\n",
    "print(model.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insted of **gradient\\_step** function, we will define an **optimizer** with learning rate and built-int **loss function**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO FIX: one block of code please\n",
    "optim = torch.optim.SGD(model.parameters(), lr=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = F.mse_loss\n",
    "loss = loss_function(model(X), Y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training the model, let's see what does the line with random cefficients look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_annotate(b, a, x, y, ann):\n",
    "    # TO FIX: it mixes objective and global matplotlib API\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(x, y)\n",
    "    y_pred = b + a * x \n",
    "    ax.plot(x, y_pred, 'g')\n",
    "    plt.xlabel(\"BodyWeight(kg)\")\n",
    "    plt.ylabel(\"BrainWeight(kg)\")\n",
    "    \n",
    "    for i, txt in enumerate(ann):\n",
    "        if i%5 == 1: \n",
    "            ax.annotate(txt, (X_viewed.numpy()[i], Y_viewed[i]))\n",
    "    plt.show()\n",
    "            \n",
    "    print('a =', a)\n",
    "    print('b =', b)\n",
    "    \n",
    "X_viewed = X.view(27)  # remove those two lines\n",
    "Y_viewed = Y.view(27)\n",
    "plot_model_annotate(model.bias.item(), model.weight.item(), X_viewed.numpy(), Y_viewed.numpy(), data.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the model - minimise the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(num_epochs, X, Y, model, loss_function, optim):\n",
    "    loss_history = []\n",
    "    preds = torch.tensor([])\n",
    "    liveloss = PlotLosses()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        Y_pred = model(X)\n",
    "        loss = loss_function(Y_pred, Y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        preds = torch.cat([preds, Y_pred], 0)\n",
    "        \n",
    "        epoch_loss = loss.data.item()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(X)\n",
    "\n",
    "        liveloss.update({\n",
    "            'loss': avg_loss,\n",
    "            'a': model.weight[0][0].item(),\n",
    "            'b': model.bias[0].item()\n",
    "        })\n",
    "        liveloss.draw()\n",
    "\n",
    "    return preds\n",
    "\n",
    "predictions = train(50, X, Y, model, loss_function, optim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see, how the line was changing during learning process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_viewed = X.view(27)\n",
    "predictions = predictions.view([-1, 27])\n",
    "plt.scatter(X, Y)\n",
    "for i in range(predictions.size()[0]):\n",
    "    if i%10 == 0:\n",
    "        plt.plot(X_viewed.numpy(),predictions[i].detach().numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we fitted the final line properly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_viewed = X.view(27)\n",
    "Y_viewed = Y.view(27)\n",
    "plot_model_annotate(model.bias.item(), model.weight.item(), X_viewed.numpy(), Y_viewed.numpy(), data.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It fits the data much better than at the begining. We have found the relation between brain and body weights among various animal species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_n = predictions[-1].exp().detach().numpy()\n",
    "df = pd.DataFrame({'Species'  : data.index, 'PredictedBrainWeight(kg)': predictions_n})\n",
    "df.index = df[\"Species\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO FIX: no, no concat; just ad a column\n",
    "data_p = pd.concat([data, df['PredictedBrainWeight(kg)']], axis=1)\n",
    "data_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compare predicted brain weights with actual data. What does it mean that the actual brain weight is bigger than predicted one? Is an animal more clever in that case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework\n",
    "\n",
    "If you want to practice linear regression, here is another dataset. It describes the relation between weight and average heart rate of various animals. \n",
    "\n",
    "**TODO: Which dataset? Don't use `data2` or anything similar as a name.**\n",
    "\n",
    "(Tip: try to scale the data, by taking logarithm of both values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv(\"data/Heart_rate_and_weight.csv\", index_col=0)\n",
    "data2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra\n",
    "\n",
    "Here are some interesting websites on the subject of linear regression:\n",
    "\n",
    "\n",
    "* [Linear regression](http://www.stat.yale.edu/Courses/1997-98/101/linreg.htm)\n",
    "* [Ordinary Least Squares Regression-Explained Visually](http://setosa.io/ev/ordinary-least-squares-regression/)\n",
    "* [Pearson correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient)\n",
    "\n",
    "\n",
    "Beware, even if there is some correlation, it may be not that sound:\n",
    "\n",
    "![https://imgs.xkcd.com/comics/linear_regression.png](https://imgs.xkcd.com/comics/linear_regression.png)\n",
    "\n",
    "![https://imgs.xkcd.com/comics/extrapolating.png](https://imgs.xkcd.com/comics/extrapolating.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "* Linear regression with more variables\n",
    "* Consequence with upper and lower indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
