{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thinking in Tensors, writing in PyTorch\n",
    "\n",
    "A hands-on course by [Piotr Migda≈Ç](https://p.migdal.pl) (2019).\n",
    "This notebook prepared by [Weronika Ormaniec](https://github.com/werkaaa).\n",
    "\n",
    "## Notebook 4: Multiple Linear Regression\n",
    "\n",
    "Simple linear regression is a useful tool when it comes to predicting an output given single predictor input. However, in practice we often come across problems which are described by more than one predictor. In this case we use Multiple Linear Regression.\n",
    "\n",
    "Instead of fitting several linear equations for each predictor, we will create one equation that will take the form:\n",
    "$$ Y = \\alpha_0 + \\alpha_1 \\cdot X_1 + \\alpha_2\\cdot X_2 + ... + \\alpha_n\\cdot X_n$$\n",
    "where $X_i$ is one of the predictors, $\\alpha_1$ is a coefficient, we want to get to know and $n$ is the number of predictors.\n",
    "\n",
    "The learning process in Multiple Linear Regression is the same as the one in Simple Linear Regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "from livelossplot import PlotLosses\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will analyze The Boston Housing Dataset. It contains information about 506 houses in Boston. There are 13 features of the houses, which have grate or little impact on the price of the house. Using PyTorch we will implement a model that will predict the prize of the house.\n",
    "\n",
    "We will take the dataset from scikit learn datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "boston_data_frame = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "boston_data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that some predictors have different orders of magnitude. That can be an obstacle during model training. That is why, we will normalize the data, so they will be in range $[-1,1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(boston.data, dtype=torch.float32)\n",
    "Y = torch.tensor(boston.target, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = torch.argmax(X, 0).type(torch.FloatTensor)\n",
    "tmp.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normalize(data):\n",
    "    data_mean = torch.mean(data, 0)\n",
    "    data_max = torch.argmax(data, 0).type(torch.FloatTensor)\n",
    "    data_min = torch.argmin(data, 0).type(torch.FloatTensor)\n",
    "    data = (data-data_mean)/(data_max-data_min)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_normalized = Normalize(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_data_frame = pd.DataFrame(np.array(X_normalized), columns=boston.feature_names)\n",
    "boston_data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we will divide the data into training and test sets because we will be able to measure how well the model is doing in general, on the examples it has not seen during training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_normalized[:400]\n",
    "Y_train = Y[:400]\n",
    "X_test = X_normalized[401:]\n",
    "Y_test = Y[401:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = nn.Linear(in_features=13, out_features=1)\n",
    "print(linear_model.weight)\n",
    "print(linear_model.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_train = linear_model(X_train)\n",
    "rmse_train = torch.sqrt(F.mse_loss(Y_train, y_predict_train))\n",
    "\n",
    "y_predict_test = linear_model(X_test)\n",
    "rmse_test = torch.sqrt(F.mse_loss(Y_test, y_predict_test))\n",
    "\n",
    "print(\"The PyTorch model performance:\")\n",
    "print('RMSE_train is {}'.format(rmse_train))\n",
    "print('RMSE_test is {}'.format(rmse_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(linear_model.parameters(), lr=0.1)\n",
    "loss_function = F.mse_loss\n",
    "loss = loss_function(linear_model(X), Y)\n",
    "print(loss)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, Y, model, loss_function, optim, num_epochs):\n",
    "    loss_history = []\n",
    "    preds = torch.tensor([])\n",
    "    liveloss = PlotLosses()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        Y_pred = model(X)\n",
    "        loss = loss_function(Y_pred, Y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        preds = torch.cat([preds, Y_pred], 0)\n",
    "        \n",
    "        epoch_loss = loss.data.item()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(X)\n",
    "\n",
    "        liveloss.update({\n",
    "            'loss': avg_loss,\n",
    "        })\n",
    "        liveloss.draw()\n",
    "    \n",
    "    return preds\n",
    "\n",
    "predictions = train(X_train, Y_train, linear_model, loss_function, optim, num_epochs=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(linear_model.weight)\n",
    "print(linear_model.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_train = linear_model(X_train)\n",
    "rmse_train = torch.sqrt(F.mse_loss(Y_train, y_predict_train))\n",
    "\n",
    "y_predict_test = linear_model(X_test)\n",
    "rmse_test = torch.sqrt(F.mse_loss(Y_test, y_predict_test))\n",
    "\n",
    "print(\"The PyTorch model performance:\")\n",
    "print('RMSE_train is {}'.format(rmse_train))\n",
    "print('RMSE_test is {}'.format(rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A we can see, our model fits the data better after training. \n",
    "\n",
    "We can now compare it with scikit learn linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_model = LinearRegression()\n",
    "lin_model.fit(np.array(X_train), np.array(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ptrain = lin_model.predict(X_train)\n",
    "rmse_tr = (np.sqrt(mean_squared_error(Y_train, y_ptrain)))\n",
    "\n",
    "y_ptest = lin_model.predict(X_test)\n",
    "rmse_te = (np.sqrt(mean_squared_error(Y_test, y_ptest)))\n",
    "\n",
    "print(\"The model performance for training set\")\n",
    "print('RMSE_train is {}'.format(rmse_tr))\n",
    "print('RMSE_test is {}'.format(rmse_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is not perfect but it has learned some intuition about the data and is able to make predictions even on the data it has not seen during learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
